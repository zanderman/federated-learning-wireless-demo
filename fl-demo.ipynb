{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"orig_nbformat":2,"kernelspec":{"name":"python392jvsc74a57bd042d0fba1dbac256883074b963d3bfc041319852b45a05dd6123e8c407a7bee86","display_name":"Python 3.9.2 64-bit ('federated-learning-wireless-demo-aWW8zZd6': pipenv)"},"metadata":{"interpreter":{"hash":"42d0fba1dbac256883074b963d3bfc041319852b45a05dd6123e8c407a7bee86"}},"colab":{"name":"fl-demo.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7216d24e9aa84f499caed23c36eb339b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_90a48636e3de490fa5cd8460e0832e98","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_57bbabc1790446ab98de684c395db6a6","IPY_MODEL_8cdef8a2ef3948968d3cb7f99a06cf1f"]}},"90a48636e3de490fa5cd8460e0832e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57bbabc1790446ab98de684c395db6a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2c30fc1f8b0c47ed9116db06f40a6c27","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b9921cd43a04162bf52640bdd1f734c"}},"8cdef8a2ef3948968d3cb7f99a06cf1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f89222f856744dba94b12b3b2ee762f6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [02:02&lt;00:00, 1389705.75it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_043f792c5028459fb3724b2bd96f141b"}},"2c30fc1f8b0c47ed9116db06f40a6c27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2b9921cd43a04162bf52640bdd1f734c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f89222f856744dba94b12b3b2ee762f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"043f792c5028459fb3724b2bd96f141b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"MVFfrCyw-KUb"},"source":["# Federated Learning Applications to Wireless Networks - A Demonstration\n","\n","Federated Learning (FL) has many advantages with applications to future wirless networks.\n","As a distributed learning technique, FL is very promising for IoT and fringe-devices where the environment is often bandwidth-limited and energy-efficiency is a premium.\n","In addition, the privacy of data collection and distribution is of growing concern with these devices maintaining sensitive information, such as real-time location data and even medical information.\n","\n","In this demonstration we will examine the benefits of using FL to train Deep Neural Networks (DNNs) in application to wireless network systems.\n","Specifically, we will see that FL is applicable in the areas of:\n","\n","- \"Green\" communications\n","- Low bandwidth environments\n","- Data privacy\n","\n","To demonstrate the advantages of low-bandwidth, \"green\", and privacy-centric communication, we will build a Convolutional Neural Network (CNN) image classifier and train using FL techniques on the traditional CIFAR10 dataset. Each image within CIFAR10 is relatively large. If network devices were required to transmit these images to a centralized server for model training there would be considerable bandwidth overhead. As such, we will show that FL can be used to lower communication overhead by training localized models that only transmit model parameters instead of the raw data over the network. In addition, because the images themselves are not being transmitted this approach is privacy-centric by nature. The global model at the central server does not need to know the details of the data, only the outcome of each localized model."]},{"cell_type":"markdown","metadata":{"id":"qz5lp2e5-KUi"},"source":["## TOC\n","\n","This demo is organized into the following sections:\n","\n","- [Setup environment](#setup-environment)\n","- [Load datasets](#load-datasets)\n","    - [CIFAR10](#cifar10)\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"89ZhK_hV-KUj"},"source":["## Setup environment"]},{"cell_type":"code","metadata":{"id":"YwOwjh5o-KUj","executionInfo":{"status":"ok","timestamp":1617917433228,"user_tz":240,"elapsed":215,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import os"],"execution_count":255,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-C90X4q-KUj","executionInfo":{"status":"ok","timestamp":1617917433352,"user_tz":240,"elapsed":333,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["workspace_root = '/fl-demo-workspace'\n","data_path = os.path.join(workspace_root, 'data')"],"execution_count":256,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8J5_eit-KUk"},"source":["## Load datasets"]},{"cell_type":"code","metadata":{"id":"ZS_P6h9l-KUk","executionInfo":{"status":"ok","timestamp":1617917433352,"user_tz":240,"elapsed":331,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":257,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6wB0D5m-KUk"},"source":["### CIFAR10\n","\n","The CIFAR10 dataset is a collection of $60,000$ images of size $32\\times32$ and $3$ color channels that are split into 10 different classes.\n","\n","The classes are represented by the set: $c \\in \\{'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\\}$"]},{"cell_type":"code","metadata":{"id":"WxCKQ0Lt-KUl","executionInfo":{"status":"ok","timestamp":1617917433484,"user_tz":240,"elapsed":460,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define label list (indices are important!)\n","labels_cifar10 = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":258,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOv2KCe1-KUl","executionInfo":{"status":"ok","timestamp":1617917433484,"user_tz":240,"elapsed":458,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define CIFAR10 image transformations for train/test sets.\n","transform_train_cifar10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","transform_test_cifar10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])"],"execution_count":259,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["7216d24e9aa84f499caed23c36eb339b","90a48636e3de490fa5cd8460e0832e98","57bbabc1790446ab98de684c395db6a6","8cdef8a2ef3948968d3cb7f99a06cf1f","2c30fc1f8b0c47ed9116db06f40a6c27","2b9921cd43a04162bf52640bdd1f734c","f89222f856744dba94b12b3b2ee762f6","043f792c5028459fb3724b2bd96f141b"]},"id":"7xeTo8Mh-KUl","executionInfo":{"status":"ok","timestamp":1617917438778,"user_tz":240,"elapsed":5750,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"dce35a27-05bc-485b-afe4-029fa0e42c4f"},"source":["# Automatically load CIFAR10 dataset into train/test sets.\n","trainset_cifar10 = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train_cifar10)\n","testset_cifar10 = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test_cifar10)"],"execution_count":260,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /fl-demo-workspace/data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7216d24e9aa84f499caed23c36eb339b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting /fl-demo-workspace/data/cifar-10-python.tar.gz to /fl-demo-workspace/data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YeojcdTv-KUm","executionInfo":{"status":"ok","timestamp":1617917438780,"user_tz":240,"elapsed":5743,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define data loaders for train/test sets.\n","trainloader_cifar10 = torch.utils.data.DataLoader(trainset_cifar10, batch_size=64, shuffle=True, num_workers=2)\n","testloader_cifar10 = torch.utils.data.DataLoader(testset_cifar10, batch_size=100, shuffle=True, num_workers=2)"],"execution_count":261,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iJYx2h--KUm"},"source":["## Define DNN Models"]},{"cell_type":"code","metadata":{"id":"_j6gyhoQ-KUn","executionInfo":{"status":"ok","timestamp":1617917438781,"user_tz":240,"elapsed":5742,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import torch\n","import torch.nn\n","import torch.optim"],"execution_count":262,"outputs":[]},{"cell_type":"code","metadata":{"id":"n40W_CNr-KUn","executionInfo":{"status":"ok","timestamp":1617917438781,"user_tz":240,"elapsed":5740,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class CIFAR10Classifier(torch.nn.Module):\n","    def __init__(self): # , w: int, h: int, n_channels: int, n_classes: int, n_conv_layers: int = 3\n","        super().__init__()\n","\n","        # conv_layers = []\n","        # in_channels = n_channels\n","        # out_channels = n_channels*2\n","        # for i in range(n_conv_layers):\n","        #     conv_layers.append(torch.nn.Conv2d(in_channels=n_channels))\n","\n","        self.block_cnn = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), padding=1), # 3x32x32 --> 63x32x32\n","            torch.nn.BatchNorm2d(num_features=32),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.MaxPool2d(kernel_size=(2,2), stride=2), # 32x32 --> 16x16\n","            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1), # 63x32x32 --> 126x16x16\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(inplace=True),\n","            # torch.nn.MaxPool2d(kernel_size=(2,2), stride=2), # 16x16 --> 8x8\n","            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=1), # 126x16x16 --> 252x8x8\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(inplace=True),\n","            # torch.nn.Conv2d(in_channels=252, out_channels=252, kernel_size=(3,3), padding=1), # 252x8x8 --> 252x8x8\n","            # torch.nn.ReLU(),\n","        )\n","\n","        in_features = 128*16*16\n","        self.block_linear = torch.nn.Sequential(\n","            # torch.nn.Linear(in_features=252*8*8, out_features=1000),\n","            # torch.nn.Linear(in_features=1000, out_features=10),\n","\n","            # torch.nn.Linear(in_features=in_features, out_features=in_features),\n","            # torch.nn.Linear(in_features=in_features, out_features=in_features),\n","            torch.nn.Linear(in_features=in_features, out_features=10),\n","        )\n","\n","    def forward(self, x):\n","\n","        # Feed input through sequential CNN/MaxPool layers.\n","        out = self.block_cnn(x)\n","\n","        # Reshape CNN-block output to fit into linear-block.\n","        # Reshapes into: (batches, channels, width, height) --> (batches, channels * width * height)\n","        out = out.view(out.size(0), -1)\n","\n","        # Feed reshaped CNN-block output into linear-block.\n","        out = self.block_linear(out) # outputs (batches, labels)\n","\n","        # Apply softmax to get label for each prediction.\n","        return out"],"execution_count":263,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMXlLYWk-KUn"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"BFaSOIgj-KUo","executionInfo":{"status":"ok","timestamp":1617917438781,"user_tz":240,"elapsed":5738,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":264,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF_cTHdm-KUo","executionInfo":{"status":"ok","timestamp":1617917438782,"user_tz":240,"elapsed":5737,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def compute_accuracy(pred, truth):\n","    \"\"\"Compute accuracy of predictions versus truth.\"\"\"\n","    pred = pred.float()\n","    truth = truth.float()\n","    return (pred == truth).sum().float()/truth.size(0)*100.0"],"execution_count":265,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PxnFVDl-KUp","executionInfo":{"status":"ok","timestamp":1617917438932,"user_tz":240,"elapsed":5885,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def compute_model_accuracy(model, loader, device='cpu'):\n","    \"\"\"Compute accuracy of a model for a given dataset loader.\"\"\"\n","    model.to(device)\n","    model.eval()\n","    ys, y_preds = [], []\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        ys.append(y)\n","        y_preds.append(torch.argmax(model(x), dim=1))\n","    \n","    y = torch.cat(ys, dim=0)\n","    y_pred = torch.cat(y_preds, dim=0)\n","    return compute_accuracy(y_pred, y)"],"execution_count":266,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6_eTecF-KUo"},"source":["### Traditional Network (not FL!)"]},{"cell_type":"code","metadata":{"id":"Pi9g1TM--KUp","executionInfo":{"status":"ok","timestamp":1617917438932,"user_tz":240,"elapsed":5884,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def train(model, loader, epoch, optim, criterion, device='cpu'):\n","    \"\"\"Helper to train a single model.\"\"\"\n","    model.to(device) # Send model to desired device.\n","    model.train() # Put the model into training mode.\n","    for e in range(epoch):\n","        running_loss = 0.0\n","        for i, data in enumerate(loader):\n","            # unpack the data, and send data to desired device.\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Zero the parameter gradients.\n","            optim.zero_grad()\n","\n","            # Evaluate the model.\n","            preds = model(inputs)\n","\n","            # Compute losses.\n","            loss = criterion(preds, labels)\n","\n","            # Back-propagate, and step the optimizer.\n","            loss.backward()\n","            optim.step()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","        # Report epoch results.\n","        print(f\"[{e}] loss: {running_loss}\")"],"execution_count":267,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yf_OtVEQ-KUp","executionInfo":{"status":"ok","timestamp":1617917438932,"user_tz":240,"elapsed":5881,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"ade21b27-a110-4d4d-c60b-6c133b798465"},"source":["# Set runtime device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":268,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZKClRtAe-KUq","executionInfo":{"status":"ok","timestamp":1617917438933,"user_tz":240,"elapsed":5872,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define traditionally-trained singular model.\n","model_trad = CIFAR10Classifier()\n","model_trad_store = os.path.join(workspace_root, 'model_trad.pt')"],"execution_count":269,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo-UkW4p-KUq","executionInfo":{"status":"ok","timestamp":1617917539623,"user_tz":240,"elapsed":106559,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"3868baff-99a0-47d8-c686-e04a0285613c"},"source":["load_from_file = False\n","\n","# Load model from store file.\n","if load_from_file and os.path.exists(model_trad_store):\n","    model_trad.load_state_dict(torch.load(model_trad_store))\n","    print(f'Loaded traditional model from file: {model_trad_store}')\n","\n","# Train model.\n","else:\n","\n","    # Learning hyperparameters.\n","    epoch = 10\n","    lr = 1e-3\n","    print(f'Training traditional model: epoch={epoch}, lr={lr}')\n","\n","    # Train the model.\n","    # Display training time too.\n","    with timecontextprint() as elapsed:\n","        optim = torch.optim.Adam(model_trad.parameters(), lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","        train(model_trad, loader=trainloader_cifar10, epoch=epoch, optim=optim, criterion=criterion, device=device)\n","\n","    # Store model state to file.\n","    torch.save(model_trad.state_dict(), model_trad_store)\n","    print(f'Saved traditional model to file: {model_trad_store}')"],"execution_count":270,"outputs":[{"output_type":"stream","text":["Training traditional model: epoch=10, lr=0.001\n","[0] loss: 1251.3343421816826\n","[1] loss: 678.6561494767666\n","[2] loss: 547.6483214199543\n","[3] loss: 449.7029107809067\n","[4] loss: 368.1199539154768\n","[5] loss: 291.6405478566885\n","[6] loss: 227.499128960073\n","[7] loss: 172.9789839796722\n","[8] loss: 132.75634049251676\n","[9] loss: 106.83524388633668\n","Elapsed time: 100.69234728813171 seconds\n","Saved traditional model to file: /fl-demo-workspace/model_trad.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zcs5L3__-KUq","executionInfo":{"status":"ok","timestamp":1617917549569,"user_tz":240,"elapsed":116495,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"b366e5a9-8e89-4a5e-b7b4-8a7a906ef5b2"},"source":["# Evaluate the model.\n","train_acc_trad = compute_model_accuracy(model_trad, trainloader_cifar10, device=device)\n","test_acc_trad = compute_model_accuracy(model_trad, testloader_cifar10, device=device)\n","print(f'Training accuracy: {train_acc_trad}, Testing accuracy: {test_acc_trad}')"],"execution_count":271,"outputs":[{"output_type":"stream","text":["Training accuracy: 95.58199310302734, Testing accuracy: 73.47999572753906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjtOxxQD_GYb","executionInfo":{"status":"ok","timestamp":1617917556346,"user_tz":240,"elapsed":123263,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"b7ec98a4-24bc-4741-d12a-c5667983ef88"},"source":["import sys\n","size_cifar10_train = sum(sys.getsizeof(img.storage()) + sys.getsizeof(lbl) for img,lbl in trainset_cifar10)\n","size_model_trad = sum(sys.getsizeof(p.storage()) for p in model_trad.parameters())\n","print(f\"CIFAR10 train images total size: {size_cifar10_train:.4e} bytes\")\n","print(f\"Traditional model parameters total size: {size_model_trad:.4e} bytes\")"],"execution_count":272,"outputs":[{"output_type":"stream","text":["CIFAR10 train images total size: 6.1938e+08 bytes\n","Traditional model parameters total size: 1.6866e+06 bytes\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GhDom41QbYCb"},"source":["As you can see, the CIFAR training dataset in its entirety uncompressed is roughly 619.38 MB, wheras the model parameters themselves are only 1.68 MB. That's a space savings by roughly 368x!"]},{"cell_type":"code","metadata":{"id":"vDvBJ1q2c4Xp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0tjPtH7J0GP"},"source":["### Federated Learning\n","\n","sources:\n","\n","- https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029"]},{"cell_type":"code","metadata":{"id":"L228MHNcJ7Uz","executionInfo":{"status":"ok","timestamp":1617917556347,"user_tz":240,"elapsed":123253,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":[""],"execution_count":272,"outputs":[]}]}