{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"metadata":{"interpreter":{"hash":"42d0fba1dbac256883074b963d3bfc041319852b45a05dd6123e8c407a7bee86"}},"colab":{"name":"fl-demo.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MVFfrCyw-KUb"},"source":["# Federated Learning Applications to Wireless Networks - A Demonstration\n","\n","Federated Learning (FL) has many advantages with applications to future wirless networks.\n","As a distributed learning technique, FL is very promising for IoT and fringe-devices where the environment is often bandwidth-limited and energy-efficiency is a premium.\n","In addition, the privacy of data collection and distribution is of growing concern with these devices maintaining sensitive information, such as real-time location data and even medical information.\n","\n","In this demonstration we will examine the benefits of using FL to train Deep Neural Networks (DNNs) in application to wireless network systems.\n","Specifically, we will see that FL is applicable in the areas of:\n","\n","- \"Green\" communications\n","- Low bandwidth environments\n","- Data privacy\n","\n","To demonstrate the advantages of low-bandwidth, \"green\", and privacy-centric communication, we will build a Convolutional Neural Network (CNN) image classifier and train using FL techniques on the traditional CIFAR10 dataset. Each image within CIFAR10 is relatively large. If network devices were required to transmit these images to a centralized server for model training there would be considerable bandwidth overhead. As such, we will show that FL can be used to lower communication overhead by training localized models that only transmit model parameters instead of the raw data over the network. In addition, because the images themselves are not being transmitted this approach is privacy-centric by nature. The global model at the central server does not need to know the details of the data, only the outcome of each localized model."]},{"cell_type":"markdown","metadata":{"id":"qz5lp2e5-KUi"},"source":["## TOC\n","\n","This demo is organized into the following sections:\n","\n","- [Setup environment](#setup-environment)\n","- [Load datasets](#load-datasets)\n","    - [CIFAR10](#cifar10)\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"89ZhK_hV-KUj"},"source":["## Setup environment"]},{"cell_type":"code","metadata":{"id":"YwOwjh5o-KUj","executionInfo":{"status":"ok","timestamp":1618192182209,"user_tz":240,"elapsed":346,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import os"],"execution_count":141,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-C90X4q-KUj","executionInfo":{"status":"ok","timestamp":1618192182546,"user_tz":240,"elapsed":675,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["workspace_root = '/fl-demo-workspace'\n","data_path = os.path.join(workspace_root, 'data')"],"execution_count":142,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8J5_eit-KUk"},"source":["## Load datasets"]},{"cell_type":"code","metadata":{"id":"ZS_P6h9l-KUk","executionInfo":{"status":"ok","timestamp":1618192182546,"user_tz":240,"elapsed":672,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":143,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6wB0D5m-KUk"},"source":["### CIFAR10\n","\n","The CIFAR10 dataset is a collection of $60,000$ images of size $32\\times32$ and $3$ color channels that are split into 10 different classes.\n","\n","The classes are represented by the set: $c \\in \\{'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\\}$"]},{"cell_type":"code","metadata":{"id":"WxCKQ0Lt-KUl","executionInfo":{"status":"ok","timestamp":1618192182547,"user_tz":240,"elapsed":670,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define label list (indices are important!)\n","labels_cifar10 = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":144,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOv2KCe1-KUl","executionInfo":{"status":"ok","timestamp":1618192182729,"user_tz":240,"elapsed":850,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define CIFAR10 image transformations for train/test sets.\n","transform_train_cifar10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","transform_test_cifar10 = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])"],"execution_count":145,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xeTo8Mh-KUl","executionInfo":{"status":"ok","timestamp":1618192184249,"user_tz":240,"elapsed":2358,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"1f721e20-d027-47c7-d4ae-638defe453fa"},"source":["# Automatically load CIFAR10 dataset into train/test sets.\n","trainset_cifar10 = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train_cifar10)\n","testset_cifar10 = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test_cifar10)"],"execution_count":146,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YeojcdTv-KUm","executionInfo":{"status":"ok","timestamp":1618192184249,"user_tz":240,"elapsed":2345,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define data loaders for train/test sets.\n","trainloader_cifar10 = torch.utils.data.DataLoader(trainset_cifar10, batch_size=64, shuffle=True, num_workers=2)\n","testloader_cifar10 = torch.utils.data.DataLoader(testset_cifar10, batch_size=100, shuffle=True, num_workers=2)"],"execution_count":147,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iJYx2h--KUm"},"source":["## Define DNN Models"]},{"cell_type":"code","metadata":{"id":"_j6gyhoQ-KUn","executionInfo":{"status":"ok","timestamp":1618192184249,"user_tz":240,"elapsed":2342,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import torch\n","import torch.nn\n","import torch.optim"],"execution_count":148,"outputs":[]},{"cell_type":"code","metadata":{"id":"n40W_CNr-KUn","executionInfo":{"status":"ok","timestamp":1618192184250,"user_tz":240,"elapsed":2340,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["class CIFAR10Classifier(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Define block of convolution/max-pool layers.\n","        self.block_cnn = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), padding=1), # 3x32x32 --> 32x32x32\n","            torch.nn.BatchNorm2d(num_features=32),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.MaxPool2d(kernel_size=(2,2), stride=2), # 32x32 --> 16x16\n","            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1), # 32x16x16 --> 64x16x16\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=1), # 64x16x16 --> 128x16x16\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(inplace=True),\n","        )\n","\n","        # Define block of fully-connected output layers.\n","        in_features = 128*16*16\n","        self.block_linear = torch.nn.Sequential(\n","            torch.nn.Linear(in_features=in_features, out_features=10),\n","        )\n","\n","    def forward(self, x):\n","\n","        # Feed input through sequential CNN/MaxPool layers.\n","        out = self.block_cnn(x)\n","\n","        # Reshape CNN-block output to fit into linear-block.\n","        # Reshapes into: (batches, channels, width, height) --> (batches, channels * width * height)\n","        out = out.view(out.size(0), -1)\n","\n","        # Feed reshaped CNN-block output into linear-block.\n","        out = self.block_linear(out) # outputs (batches, labels)\n","\n","        # Apply softmax to get label for each prediction.\n","        return out"],"execution_count":149,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMXlLYWk-KUn"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"BFaSOIgj-KUo","executionInfo":{"status":"ok","timestamp":1618192184250,"user_tz":240,"elapsed":2337,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["import time\n","from contextlib import contextmanager\n","\n","class timecontext:\n","    \"\"\"Elapsed time context manager.\"\"\"\n","    def __enter__(self):\n","        self.seconds = time.time()\n","        return self\n","    \n","    def __exit__(self, type, value, traceback):\n","        self.seconds = time.time() - self.seconds\n","\n","@contextmanager\n","def timecontextprint(description='Elapsed time'):\n","    \"\"\"Context manager to print elapsed time from call.\"\"\"\n","    with timecontext() as t:\n","        yield t\n","    print(f\"{description}: {t.seconds} seconds\")"],"execution_count":150,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PxnFVDl-KUp","executionInfo":{"status":"ok","timestamp":1618192184251,"user_tz":240,"elapsed":2332,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def compute_model_accuracy(model, loader, device='cpu'):\n","    \"\"\"Compute accuracy of a model for a given dataset loader.\"\"\"\n","    model.to(device)\n","    model.eval()\n","    ys, y_preds = [], []\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            ys.append(y)\n","            y_preds.append(torch.argmax(model(x), dim=1))\n","    \n","    y = torch.cat(ys, dim=0).float()\n","    y_pred = torch.cat(y_preds, dim=0).float()\n","    return (y_pred == y).sum().float()/y.size(0)*100.0"],"execution_count":152,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6_eTecF-KUo"},"source":["### Traditional Network (not FL!)\n","\n","Here we will traditionally train the CIFAR10 classifier on all the training data. We will then evaluate the performance of the model on the test data.\n","\n","Traditional training involves evaluating a single model on a sequence of \"training\" data, where the desired outcomes are known. The losses of the computed output versus the actual values are then used to update the parameters of the model.\n","\n","Specifically, the sequence of training operations for each epoch are:\n","\n","1. Load a batch of training data\n","2. Zero the model parameter gradients\n","3. Evaluate the model on the training data inputs\n","4. Compare the evaluation result with the known truth\n","5. Back-propagate the comparison (a.k.a. \"loss\") through the network to update model parameters"]},{"cell_type":"code","metadata":{"id":"Pi9g1TM--KUp","executionInfo":{"status":"ok","timestamp":1618192184251,"user_tz":240,"elapsed":2329,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["def train(model, loader, epoch, optim, criterion, device='cpu', verbose=True) -> float:\n","    \"\"\"Helper to train a single model.\n","    \n","    Returns an iterator to the running loss for each training epoch.\n","    \"\"\"\n","    model.to(device) # Send model to desired device.\n","    model.train() # Put the model into training mode.\n","    for e in range(epoch):\n","        running_loss = 0.0\n","        for i, data in enumerate(loader):\n","            # unpack the data, and send data to desired device.\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Zero the parameter gradients.\n","            optim.zero_grad()\n","\n","            # Evaluate the model.\n","            preds = model(inputs)\n","\n","            # Compute losses.\n","            loss = criterion(preds, labels)\n","\n","            # Back-propagate, and step the optimizer.\n","            loss.backward()\n","            optim.step()\n","\n","            # Accumulate the loss for this epoch.\n","            running_loss += loss.item()\n","\n","        # Report the loss per epoch if necessary.\n","        if verbose: print(f\"[epoch {e}] loss: {running_loss}\")\n","\n","    # Return the last-computed loss.\n","    return loss.item()"],"execution_count":153,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZV9zGWbYB-wl"},"source":["To speed-up training PyTorch allows us to leverage a GPU, using CUDA, if one is available. Since training a CNN can be computationally intensive we prefer to use a GPU for speed, but will revert to using the CPU if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yf_OtVEQ-KUp","executionInfo":{"status":"ok","timestamp":1618192184634,"user_tz":240,"elapsed":2700,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"dc6759a3-9071-4d9a-8fc2-c05d12c6bb26"},"source":["# Set runtime device.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")"],"execution_count":154,"outputs":[{"output_type":"stream","text":["Device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZKClRtAe-KUq","executionInfo":{"status":"ok","timestamp":1618192184634,"user_tz":240,"elapsed":2687,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define traditionally-trained singular model.\n","model_trad = CIFAR10Classifier()\n","model_trad_store = os.path.join(workspace_root, 'model_trad.pt')"],"execution_count":155,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3btBGXdCg6z"},"source":["Here we define a set of training learning parameters. We define the number of training epochs `epochs_trad` (i.e., the number of times the model is evaluated on the training data) and the learning rate `lr_trad` which defines the step size of the optimizer."]},{"cell_type":"code","metadata":{"id":"iPK1_xKnBAZK","executionInfo":{"status":"ok","timestamp":1618192184635,"user_tz":240,"elapsed":2685,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Learning hyperparameters.\n","epochs_trad = 10\n","lr_trad = 1e-3"],"execution_count":156,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7NKnqFXVDENK"},"source":["In this block we actually perform the traditional training.\n","\n","We choose the `Adam` parameter optimzer, which is widely used in practice, and `CrossEntropyLoss` which is useful for training classification models with a discrete set of classes.\n","\n","At every training epoch the running loss will be reported. This helps us get a sense for how the model is learning. Ideally we want to see decreasing loss over all the epochs. Recall that loss is the comparison between the model predicitons versus the actual values. So, low losses mean the predictions are very close to the actual values.\n","\n","To speed-up subsequent runs, we also save the trained model to a file. This allows us to train the traditional model once, and then simply load the pre-trained model (using the flag `load_from_file`) if the Jupyter notebook is run multiple times during a session."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo-UkW4p-KUq","executionInfo":{"status":"ok","timestamp":1618192293283,"user_tz":240,"elapsed":111321,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"47a8339e-e186-42b3-e5da-9e18f5a1f55c"},"source":["load_from_file = True\n","\n","# Load model from store file.\n","if load_from_file and os.path.exists(model_trad_store):\n","    model_trad.load_state_dict(torch.load(model_trad_store))\n","    print(f'Loaded traditional model from file: {model_trad_store}')\n","\n","# Train model.\n","else:\n","    print(f'Training traditional model: epoch={epochs_trad}, lr={lr_trad}')\n","\n","    # Train the model.\n","    # Display training time too.\n","    with timecontextprint() as elapsed:\n","        optim = torch.optim.Adam(model_trad.parameters(), lr=lr_trad)\n","        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","        train(model_trad, loader=trainloader_cifar10, epoch=epochs_trad, optim=optim, criterion=criterion, device=device)\n","\n","    # Store model state to file.\n","    torch.save(model_trad.state_dict(), model_trad_store)\n","    print(f'Saved traditional model to file: {model_trad_store}')"],"execution_count":157,"outputs":[{"output_type":"stream","text":["Training traditional model: epoch=10, lr=0.001\n","[epoch 0] loss: 1218.0067186951637\n","[epoch 1] loss: 693.181870341301\n","[epoch 2] loss: 559.9090529382229\n","[epoch 3] loss: 473.1813815534115\n","[epoch 4] loss: 385.8151903450489\n","[epoch 5] loss: 310.21438404917717\n","[epoch 6] loss: 237.25656838715076\n","[epoch 7] loss: 182.90891812369227\n","[epoch 8] loss: 135.4885181710124\n","[epoch 9] loss: 108.62444218620658\n","Elapsed time: 108.92792224884033 seconds\n","Saved traditional model to file: /fl-demo-workspace/model_trad.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4n61M1DLFKuN"},"source":["Now we evaluate the traditionally-trained model on both the training data and the testing data. This summarizes the performance of the model after training and allows us to see how well our model is able to \"generalize\" on unseen data.\n","\n","You should see a training accuracy of about 95% and a testing accuracy of about 74%."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zcs5L3__-KUq","executionInfo":{"status":"ok","timestamp":1618192303943,"user_tz":240,"elapsed":121960,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"d8f7c39e-934a-4d92-e5c0-1949c2de1d81"},"source":["# Evaluate the model.\n","train_acc_trad = compute_model_accuracy(model_trad, trainloader_cifar10, device=device)\n","test_acc_trad = compute_model_accuracy(model_trad, testloader_cifar10, device=device)\n","print(f'Training accuracy: {train_acc_trad}, Testing accuracy: {test_acc_trad}')"],"execution_count":158,"outputs":[{"output_type":"stream","text":["Training accuracy: 97.21599578857422, Testing accuracy: 74.27999877929688\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UpKK1J5iFuwQ"},"source":["In the cell below we compute the total size of the training image dataset (in bytes) and also compute the total size of the model parameters (in bytes)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjtOxxQD_GYb","executionInfo":{"status":"ok","timestamp":1618192327720,"user_tz":240,"elapsed":7959,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"b881ef14-8f74-4fd0-b9f4-495ba7c5b5f4"},"source":["import sys\n","size_cifar10_train = sum(sys.getsizeof(img.storage()) + sys.getsizeof(lbl) for img,lbl in trainset_cifar10)\n","size_model_trad = sum(sys.getsizeof(p.storage()) for p in model_trad.parameters())\n","print(f\"CIFAR10 train images total size: {size_cifar10_train:.4e} bytes\")\n","print(f\"Traditional model parameters total size: {size_model_trad:.4e} bytes\")"],"execution_count":159,"outputs":[{"output_type":"stream","text":["CIFAR10 train images total size: 6.1938e+08 bytes\n","Traditional model parameters total size: 1.6866e+06 bytes\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GhDom41QbYCb"},"source":["As you can see, the CIFAR training dataset in its entirety uncompressed is roughly 619.38 MB, wheras the model parameters themselves are only 1.68 MB. That's a space savings by roughly 368x!\n","\n","Circling back on the applicability of DNNs and wireless networks we must examine the overhead necessary to train a model. Specifically, bandwidth and spectrum usage are of chief concern as highly valuable resources. Training this classificaiton model required the use of 50,000 training images, which as seen above weighs in at 619.38 MB in size. \n","\n","To traditionally train this model in a wireless network, the data would need to be collected from various devices and then transmitted over the network to a central server. These large data transfers are an inefficient use of resources and time consuming for low-bandwidth networks."]},{"cell_type":"markdown","metadata":{"id":"c0tjPtH7J0GP"},"source":["### Federated Learning\n","\n","Here we will train another CIFAR10 image classifer, but this time we will use federated learning techniques to distribute the training.\n","\n","In federated learning, we train a \"global\" model by aggregating the parameters of several \"client\" models. This technique distributes the computational complexity of training to multiple client devices and also mitigates the large-quantity data transfer as required by the traditional method. It does this by aggregating the parameters of each client model at regular update intervals.\n","\n","The training steps in federated learning are actually very similar to the traditional case. The difference\n","\n","sources:\n","\n","- https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029"]},{"cell_type":"markdown","metadata":{"id":"milb_5baKa_U"},"source":["#### Parameter Aggregation\n","\n","The technique behind federated learning lies in how the parameters of the client models are aggregated into the global model.\n","\n","In cellular networks, speech vocoders save bandwidth by mathematically modeling speech bits using a set of parameters. These parameters are then transmitted over the network to reconstruct the speech signal rather than the actual speech bits themselves. Likewise, federated learning reduces the amount of data required to train the global model by **locally modeling the local data** (i.e., the client model) and then using the **local model parameters** to update the global model. These parameters are often much smaller in size than the amount of data used to train them and is thus more efficient in terms of communication overhead.\n","\n","It is important to note that in federated learning only the client models actually undergo training. The global model only receives parameter aggregation updates from the clients and does not undergo any training itself.\n","\n","There are many types of parameter aggregation methods (i.e. min, max, mean, weighted mean, etc.). The type to use varies depending on the modeling task at hand. In our case, the simple mean of the parameters will suffice.\n","\n","In the code block below we define a global aggregation function `global_aggregate` which does the following:\n","\n","1. Aggregates the parameters of several client models\n","2. Updates the parameters of a given global model to these aggregations\n","3. Synchronizes all client models to the new global model parameters"]},{"cell_type":"code","metadata":{"id":"L228MHNcJ7Uz","executionInfo":{"status":"ok","timestamp":1618192414759,"user_tz":240,"elapsed":205,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["from typing import List\n","\n","def global_aggregate(global_model: torch.nn.Module, client_models: List[torch.nn.Module]):\n","    \"\"\"Aggregate the parameters from several client models to a global model.\n","\n","    The aggregation technique is the 'mean' of each parameter over all clients.\n","    \"\"\"\n","\n","    # Get current global model parameters.\n","    global_state_dict = global_model.state_dict()\n","\n","    # Aggregate client model parameters using 'mean'.\n","    for key in global_state_dict.keys():\n","        global_state_dict[key] = torch.stack([client_models[i].state_dict()[key].float() for i in range(len(client_models))], dim=0).mean(dim=0)\n","    \n","    # Update the global model using aggregated parameters.\n","    global_model.load_state_dict(global_state_dict)\n","\n","    # Finally, update each client's parameters to be the same as the global model.\n","    for model in client_models:\n","        model.load_state_dict(global_state_dict)"],"execution_count":160,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhH2GOx9B4sH"},"source":["Here we define the hyperparameters of the federated learning technique. To keep the comparison with traditional model fair we will train the \"global\" model using the same number of epochs (in this case called a \"round\"). Likewise, each client model will be trained using the same number of epochs. This way each client is trained, and the global model is updated, the same number of times as the traditional model.\n","\n","The number of clients to use here is representative of a wireless network of devices."]},{"cell_type":"code","metadata":{"id":"gESrj1Q6uqjt","executionInfo":{"status":"ok","timestamp":1618192417232,"user_tz":240,"elapsed":205,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Federated learning hyper-parameters.\n","n_clients = 20 # Number of client models.\n","n_rounds = 10 # Number of training aggregation rounds.\n","epochs_fl = 10 # Number of training epochs for each client.\n","lr_fl = 1e-3 # Client optimizer learning rate."],"execution_count":161,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsPEcfT8NsWb"},"source":["Next we define the global and client models, and initially synchronize their parameter values (important in the case of random value initialization).\n","\n","We then define a set of parameter optimizers for each of the client models and a common loss criterion. It is important to note that we are using the same `Adam` optimizer and `CrossEntropyLoss` criterion methods as in the traditional training case to fairly evaluate the performance at the end."]},{"cell_type":"code","metadata":{"id":"ECMmCezSt3AY","executionInfo":{"status":"ok","timestamp":1618192418427,"user_tz":240,"elapsed":353,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Define the global model.\n","model_fl_global = CIFAR10Classifier()\n","\n","# Define client models\n","model_fl_clients = [CIFAR10Classifier() for _ in range(n_clients)]\n","\n","# Initially synchronize clients to the global model.\n","for model in model_fl_clients:\n","    model.load_state_dict(model_fl_global.state_dict())\n","\n","# Define training optimizers for each client model.\n","optimizers_fl = [torch.optim.Adam(model.parameters(), lr=lr_fl) for model in model_fl_clients]\n","\n","# Define training criteria.\n","criterion = torch.nn.CrossEntropyLoss(reduction='mean')"],"execution_count":162,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dQJRM8KzBXCe"},"source":["We do not want to train all the clients on the same data, which would introduce heavy bias into the model. To mitigate this with our CIFAR10 dataset we can split the training data randomly into equal-length sets amongst the clients so that there is no overlap."]},{"cell_type":"code","metadata":{"id":"gs5lASXSzvJp","executionInfo":{"status":"ok","timestamp":1618192421627,"user_tz":240,"elapsed":433,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}}},"source":["# Split dataset amongst clients so that no training data overlaps.\n","trainset_cifar10_fl_len = trainset_cifar10.data.shape[0]//n_clients\n","trainset_cifar10_fl_splits = torch.utils.data.random_split(trainset_cifar10, lengths=[trainset_cifar10_fl_len]*n_clients)\n","\n","# Create data loaders for each client, which iterates over each split.\n","trainset_cifar10_fl_loaders = [torch.utils.data.DataLoader(split, batch_size=32, shuffle=True) for split in trainset_cifar10_fl_splits]"],"execution_count":163,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s92b58RMPQI3"},"source":["Now it is time to actually train the global model using federated learning. For each aggregation round each client model is trained using its own unique training dataset using exactly the same training steps as in the traditional case, calling the `train` function. Once each client has been trained the parameters from all clients are aggregated into the global model and then synchronized back to each client using the `global_aggregate` function. The losses from each client training session, the running loss for each aggregation round, and the overall training and testing accuracy of the global model are reported to get a sence of how the various models are learning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As0X9zyxudPm","executionInfo":{"status":"ok","timestamp":1618193927853,"user_tz":240,"elapsed":1495640,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"63439819-1ed0-40c7-bade-fd6deba37e45"},"source":["import numpy as np\n","\n","# Run training/aggregation rounds.\n","for round in range(n_rounds):\n","\n","    with timecontext() as t_round:\n","        loss_round = 0.0\n","        for c in range(n_clients):\n","            with timecontext() as t_client:\n","                loss = train(model_fl_clients[c],\n","                    loader=trainset_cifar10_fl_loaders[c],\n","                    epoch=epochs_fl,\n","                    optim=optimizers_fl[c],\n","                    criterion=criterion,\n","                    device=device,\n","                    verbose=False,\n","                )\n","                loss_round += loss\n","            print(f\"[round {round}] client: {c}, loss: {loss}, elapsed: {t_client.seconds} seconds\")\n","    print(f\"[round {round}] loss: {loss_round}, elapsed: {t_round.seconds} seconds\")\n","\n","    # Aggregate the parameters for each model to the global one.\n","    global_aggregate(model_fl_global, model_fl_clients)\n","\n","    # Evaluate the global model.\n","    train_acc_fl = compute_model_accuracy(model_fl_global, trainloader_cifar10, device=device)\n","    test_acc_fl = compute_model_accuracy(model_fl_global, testloader_cifar10, device=device)\n","    print(f'[round {round}] Training accuracy: {train_acc_fl}, Testing accuracy: {test_acc_fl}')"],"execution_count":164,"outputs":[{"output_type":"stream","text":["[round 0] client: 0, loss: 0.00465038139373064, elapsed: 7.082516670227051 seconds\n","[round 0] client: 1, loss: 0.08840160071849823, elapsed: 7.078489780426025 seconds\n","[round 0] client: 2, loss: 1.8217999935150146, elapsed: 7.105413436889648 seconds\n","[round 0] client: 3, loss: 0.18811330199241638, elapsed: 7.1055567264556885 seconds\n","[round 0] client: 4, loss: 0.010464943014085293, elapsed: 7.03026819229126 seconds\n","[round 0] client: 5, loss: 0.05710653215646744, elapsed: 7.006112813949585 seconds\n","[round 0] client: 6, loss: 0.015502849593758583, elapsed: 7.029476881027222 seconds\n","[round 0] client: 7, loss: 0.009899778291583061, elapsed: 6.984573602676392 seconds\n","[round 0] client: 8, loss: 0.014724464155733585, elapsed: 7.002286195755005 seconds\n","[round 0] client: 9, loss: 0.0328194722533226, elapsed: 6.979635238647461 seconds\n","[round 0] client: 10, loss: 0.014924203045666218, elapsed: 6.919126749038696 seconds\n","[round 0] client: 11, loss: 0.32284167408943176, elapsed: 7.040960788726807 seconds\n","[round 0] client: 12, loss: 0.012729664333164692, elapsed: 7.075142860412598 seconds\n","[round 0] client: 13, loss: 0.032942939549684525, elapsed: 7.093879461288452 seconds\n","[round 0] client: 14, loss: 0.36729562282562256, elapsed: 7.144254446029663 seconds\n","[round 0] client: 15, loss: 0.010683140717446804, elapsed: 7.040496110916138 seconds\n","[round 0] client: 16, loss: 0.6989317536354065, elapsed: 7.096932649612427 seconds\n","[round 0] client: 17, loss: 0.44644463062286377, elapsed: 7.043434143066406 seconds\n","[round 0] client: 18, loss: 0.16467416286468506, elapsed: 7.02888822555542 seconds\n","[round 0] client: 19, loss: 0.2607768476009369, elapsed: 7.080226898193359 seconds\n","[round 0] loss: 4.575727956369519, elapsed: 140.97005581855774 seconds\n","[round 0] Training accuracy: 56.85399627685547, Testing accuracy: 53.63999557495117\n","[round 1] client: 0, loss: 0.001242607133463025, elapsed: 7.0982396602630615 seconds\n","[round 1] client: 1, loss: 0.019608445465564728, elapsed: 6.948335409164429 seconds\n","[round 1] client: 2, loss: 0.01339064259082079, elapsed: 7.037212371826172 seconds\n","[round 1] client: 3, loss: 0.011253802105784416, elapsed: 7.07958984375 seconds\n","[round 1] client: 4, loss: 0.05340665578842163, elapsed: 7.0771191120147705 seconds\n","[round 1] client: 5, loss: 0.002771225990727544, elapsed: 7.147979497909546 seconds\n","[round 1] client: 6, loss: 0.0018860769923776388, elapsed: 6.933544635772705 seconds\n","[round 1] client: 7, loss: 0.0027997042052447796, elapsed: 7.010648965835571 seconds\n","[round 1] client: 8, loss: 0.040004536509513855, elapsed: 6.871382236480713 seconds\n","[round 1] client: 9, loss: 0.01504997443407774, elapsed: 7.025418996810913 seconds\n","[round 1] client: 10, loss: 0.008758967742323875, elapsed: 6.910348176956177 seconds\n","[round 1] client: 11, loss: 0.005894508678466082, elapsed: 6.981614351272583 seconds\n","[round 1] client: 12, loss: 0.020933065563440323, elapsed: 6.932722806930542 seconds\n","[round 1] client: 13, loss: 0.0005700993351638317, elapsed: 7.003148555755615 seconds\n","[round 1] client: 14, loss: 0.002539795357733965, elapsed: 6.898422956466675 seconds\n","[round 1] client: 15, loss: 0.01563260145485401, elapsed: 6.917082071304321 seconds\n","[round 1] client: 16, loss: 0.003714290214702487, elapsed: 7.0182671546936035 seconds\n","[round 1] client: 17, loss: 0.0006020937580615282, elapsed: 7.022817850112915 seconds\n","[round 1] client: 18, loss: 0.011966615915298462, elapsed: 7.086911678314209 seconds\n","[round 1] client: 19, loss: 0.0013358350843191147, elapsed: 6.958191633224487 seconds\n","[round 1] loss: 0.23336154432035983, elapsed: 139.9635238647461 seconds\n","[round 1] Training accuracy: 69.50799560546875, Testing accuracy: 64.6500015258789\n","[round 2] client: 0, loss: 0.02748066931962967, elapsed: 7.003131866455078 seconds\n","[round 2] client: 1, loss: 0.00565676623955369, elapsed: 6.965879917144775 seconds\n","[round 2] client: 2, loss: 0.005434059537947178, elapsed: 6.989136695861816 seconds\n","[round 2] client: 3, loss: 0.038940418511629105, elapsed: 6.854974269866943 seconds\n","[round 2] client: 4, loss: 0.0048437295481562614, elapsed: 6.821508169174194 seconds\n","[round 2] client: 5, loss: 0.0044723693281412125, elapsed: 6.882753849029541 seconds\n","[round 2] client: 6, loss: 0.0075943851843476295, elapsed: 7.047589302062988 seconds\n","[round 2] client: 7, loss: 0.0002896888181567192, elapsed: 6.933787107467651 seconds\n","[round 2] client: 8, loss: 0.005628384184092283, elapsed: 6.909910202026367 seconds\n","[round 2] client: 9, loss: 0.002128619235008955, elapsed: 6.955075740814209 seconds\n","[round 2] client: 10, loss: 0.0029639590065926313, elapsed: 6.830729961395264 seconds\n","[round 2] client: 11, loss: 0.008243439719080925, elapsed: 6.924607515335083 seconds\n","[round 2] client: 12, loss: 0.0031176065094769, elapsed: 7.0095055103302 seconds\n","[round 2] client: 13, loss: 0.7954403162002563, elapsed: 6.943472862243652 seconds\n","[round 2] client: 14, loss: 0.6523215770721436, elapsed: 6.907075643539429 seconds\n","[round 2] client: 15, loss: 0.0010152964387089014, elapsed: 6.872515439987183 seconds\n","[round 2] client: 16, loss: 0.0033004109282046556, elapsed: 7.014093399047852 seconds\n","[round 2] client: 17, loss: 0.009748456999659538, elapsed: 7.055073976516724 seconds\n","[round 2] client: 18, loss: 0.00444689579308033, elapsed: 6.970746278762817 seconds\n","[round 2] client: 19, loss: 0.0030197002924978733, elapsed: 6.924704313278198 seconds\n","[round 2] loss: 1.5860867488663644, elapsed: 138.81927394866943 seconds\n","[round 2] Training accuracy: 73.40399932861328, Testing accuracy: 67.68000030517578\n","[round 3] client: 0, loss: 0.00047962702228687704, elapsed: 6.970855236053467 seconds\n","[round 3] client: 1, loss: 0.0007138736546039581, elapsed: 7.0678391456604 seconds\n","[round 3] client: 2, loss: 0.14551183581352234, elapsed: 6.877701997756958 seconds\n","[round 3] client: 3, loss: 0.004933903459459543, elapsed: 6.981075286865234 seconds\n","[round 3] client: 4, loss: 0.04790743067860603, elapsed: 7.0013251304626465 seconds\n","[round 3] client: 5, loss: 0.004640450235456228, elapsed: 7.014299631118774 seconds\n","[round 3] client: 6, loss: 0.0011454133782535791, elapsed: 6.953133583068848 seconds\n","[round 3] client: 7, loss: 0.0004620553518179804, elapsed: 7.0753700733184814 seconds\n","[round 3] client: 8, loss: 0.0009731970494613051, elapsed: 7.168869256973267 seconds\n","[round 3] client: 9, loss: 0.02168964594602585, elapsed: 7.020830392837524 seconds\n","[round 3] client: 10, loss: 0.006030305288732052, elapsed: 6.94840931892395 seconds\n","[round 3] client: 11, loss: 0.006376606412231922, elapsed: 6.998795986175537 seconds\n","[round 3] client: 12, loss: 1.4900885616953019e-05, elapsed: 6.982983827590942 seconds\n","[round 3] client: 13, loss: 0.0020104891154915094, elapsed: 7.0702643394470215 seconds\n","[round 3] client: 14, loss: 0.00029564619762822986, elapsed: 7.065815210342407 seconds\n","[round 3] client: 15, loss: 0.011427970603108406, elapsed: 6.933510065078735 seconds\n","[round 3] client: 16, loss: 0.016245825216174126, elapsed: 6.96539306640625 seconds\n","[round 3] client: 17, loss: 0.01016510371118784, elapsed: 7.147576332092285 seconds\n","[round 3] client: 18, loss: 0.0012740034144371748, elapsed: 7.151677370071411 seconds\n","[round 3] client: 19, loss: 0.004160860553383827, elapsed: 7.037511348724365 seconds\n","[round 3] loss: 0.28645914398748573, elapsed: 140.43569660186768 seconds\n","[round 3] Training accuracy: 75.71399688720703, Testing accuracy: 69.19999694824219\n","[round 4] client: 0, loss: 0.002251714700832963, elapsed: 6.987778425216675 seconds\n","[round 4] client: 1, loss: 0.004519691225141287, elapsed: 6.922941446304321 seconds\n","[round 4] client: 2, loss: 0.003399557899683714, elapsed: 6.899179220199585 seconds\n","[round 4] client: 3, loss: 0.003474969882518053, elapsed: 6.879415273666382 seconds\n","[round 4] client: 4, loss: 0.008419377729296684, elapsed: 6.943626642227173 seconds\n","[round 4] client: 5, loss: 0.0031106113456189632, elapsed: 7.045098304748535 seconds\n","[round 4] client: 6, loss: 0.0029761295299977064, elapsed: 6.96387505531311 seconds\n","[round 4] client: 7, loss: 0.00037954110302962363, elapsed: 7.002904653549194 seconds\n","[round 4] client: 8, loss: 0.007446214556694031, elapsed: 6.893749713897705 seconds\n","[round 4] client: 9, loss: 0.000321231345878914, elapsed: 6.892183542251587 seconds\n","[round 4] client: 10, loss: 0.0004856794257648289, elapsed: 6.8900229930877686 seconds\n","[round 4] client: 11, loss: 0.0030709540005773306, elapsed: 6.865550518035889 seconds\n","[round 4] client: 12, loss: 0.00039637155714444816, elapsed: 6.878474235534668 seconds\n","[round 4] client: 13, loss: 0.0010913750156760216, elapsed: 6.912983655929565 seconds\n","[round 4] client: 14, loss: 0.0016023549251258373, elapsed: 6.87970757484436 seconds\n","[round 4] client: 15, loss: 0.0002075046650134027, elapsed: 6.937537431716919 seconds\n","[round 4] client: 16, loss: 0.003793352749198675, elapsed: 6.982558012008667 seconds\n","[round 4] client: 17, loss: 0.19912678003311157, elapsed: 6.864196062088013 seconds\n","[round 4] client: 18, loss: 0.05347021669149399, elapsed: 6.9432761669158936 seconds\n","[round 4] client: 19, loss: 0.0010932013392448425, elapsed: 6.826103448867798 seconds\n","[round 4] loss: 0.3006368297210429, elapsed: 138.4140727519989 seconds\n","[round 4] Training accuracy: 77.6500015258789, Testing accuracy: 70.15999603271484\n","[round 5] client: 0, loss: 0.008710579015314579, elapsed: 6.966683864593506 seconds\n","[round 5] client: 1, loss: 0.0022645886056125164, elapsed: 6.931363582611084 seconds\n","[round 5] client: 2, loss: 0.007991023361682892, elapsed: 6.882292747497559 seconds\n","[round 5] client: 3, loss: 0.0010972797172144055, elapsed: 6.954475402832031 seconds\n","[round 5] client: 4, loss: 0.002190249040722847, elapsed: 6.838217496871948 seconds\n","[round 5] client: 5, loss: 0.01170677226036787, elapsed: 6.923969030380249 seconds\n","[round 5] client: 6, loss: 0.0016806598287075758, elapsed: 6.853599548339844 seconds\n","[round 5] client: 7, loss: 0.00010232914064545184, elapsed: 6.807098865509033 seconds\n","[round 5] client: 8, loss: 0.001062163501046598, elapsed: 6.971271514892578 seconds\n","[round 5] client: 9, loss: 0.0003192292933817953, elapsed: 7.146313667297363 seconds\n","[round 5] client: 10, loss: 0.0069260806776583195, elapsed: 6.969423532485962 seconds\n","[round 5] client: 11, loss: 0.0016858680173754692, elapsed: 6.947647333145142 seconds\n","[round 5] client: 12, loss: 0.009271858260035515, elapsed: 6.844343662261963 seconds\n","[round 5] client: 13, loss: 0.0006092855473980308, elapsed: 6.931147575378418 seconds\n","[round 5] client: 14, loss: 0.00045224608038552105, elapsed: 6.823559761047363 seconds\n","[round 5] client: 15, loss: 0.0008829449070617557, elapsed: 6.83670711517334 seconds\n","[round 5] client: 16, loss: 0.004932090640068054, elapsed: 6.851932764053345 seconds\n","[round 5] client: 17, loss: 0.00210848031565547, elapsed: 6.990240097045898 seconds\n","[round 5] client: 18, loss: 0.00011055257346015424, elapsed: 6.973440170288086 seconds\n","[round 5] client: 19, loss: 0.004983311519026756, elapsed: 6.780926942825317 seconds\n","[round 5] loss: 0.06908759230282158, elapsed: 138.22698259353638 seconds\n","[round 5] Training accuracy: 79.17599487304688, Testing accuracy: 70.87000274658203\n","[round 6] client: 0, loss: 0.00026517879450693727, elapsed: 6.925362586975098 seconds\n","[round 6] client: 1, loss: 0.012887347489595413, elapsed: 7.068193197250366 seconds\n","[round 6] client: 2, loss: 0.0012378632090985775, elapsed: 6.921527624130249 seconds\n","[round 6] client: 3, loss: 0.005474065896123648, elapsed: 6.930283069610596 seconds\n","[round 6] client: 4, loss: 0.06762613356113434, elapsed: 6.867732048034668 seconds\n","[round 6] client: 5, loss: 5.432466423371807e-05, elapsed: 6.8041980266571045 seconds\n","[round 6] client: 6, loss: 0.010261786170303822, elapsed: 6.914733409881592 seconds\n","[round 6] client: 7, loss: 6.496853529824875e-06, elapsed: 6.858607769012451 seconds\n","[round 6] client: 8, loss: 0.00035107199801132083, elapsed: 6.836579322814941 seconds\n","[round 6] client: 9, loss: 0.004165531601756811, elapsed: 6.941730499267578 seconds\n","[round 6] client: 10, loss: 0.00842435285449028, elapsed: 6.874747276306152 seconds\n","[round 6] client: 11, loss: 0.0015139604220166802, elapsed: 6.803118705749512 seconds\n","[round 6] client: 12, loss: 0.004957364872097969, elapsed: 6.749122619628906 seconds\n","[round 6] client: 13, loss: 0.00029685813933610916, elapsed: 6.785914659500122 seconds\n","[round 6] client: 14, loss: 0.0018560073804110289, elapsed: 6.944775104522705 seconds\n","[round 6] client: 15, loss: 0.0210303645581007, elapsed: 6.81871485710144 seconds\n","[round 6] client: 16, loss: 0.00030554388649761677, elapsed: 6.943349599838257 seconds\n","[round 6] client: 17, loss: 0.006177926901727915, elapsed: 6.870844602584839 seconds\n","[round 6] client: 18, loss: 0.00021388009190559387, elapsed: 6.851536989212036 seconds\n","[round 6] client: 19, loss: 0.0025146782863885164, elapsed: 6.855541706085205 seconds\n","[round 6] loss: 0.14962073763126682, elapsed: 137.56905102729797 seconds\n","[round 6] Training accuracy: 80.6780014038086, Testing accuracy: 71.4000015258789\n","[round 7] client: 0, loss: 0.001572056906297803, elapsed: 6.820976257324219 seconds\n","[round 7] client: 1, loss: 0.00044605007860809565, elapsed: 6.932955265045166 seconds\n","[round 7] client: 2, loss: 0.002664695493876934, elapsed: 6.853401184082031 seconds\n","[round 7] client: 3, loss: 0.04418335109949112, elapsed: 6.948273420333862 seconds\n","[round 7] client: 4, loss: 0.00036614073906093836, elapsed: 6.941120624542236 seconds\n","[round 7] client: 5, loss: 0.0876292958855629, elapsed: 6.960598468780518 seconds\n","[round 7] client: 6, loss: 0.03171853721141815, elapsed: 6.957571268081665 seconds\n","[round 7] client: 7, loss: 0.0028720383998006582, elapsed: 6.788206577301025 seconds\n","[round 7] client: 8, loss: 0.0001954446779564023, elapsed: 6.960515975952148 seconds\n","[round 7] client: 9, loss: 0.0002007796138059348, elapsed: 6.861684322357178 seconds\n","[round 7] client: 10, loss: 0.0022179665975272655, elapsed: 6.904324769973755 seconds\n","[round 7] client: 11, loss: 0.0011117419926449656, elapsed: 6.9694201946258545 seconds\n","[round 7] client: 12, loss: 0.0025408901274204254, elapsed: 6.923965215682983 seconds\n","[round 7] client: 13, loss: 0.010199048556387424, elapsed: 6.933815002441406 seconds\n","[round 7] client: 14, loss: 0.06614062190055847, elapsed: 6.818634748458862 seconds\n","[round 7] client: 15, loss: 0.0009988595265895128, elapsed: 6.857150316238403 seconds\n","[round 7] client: 16, loss: 0.00038656729157082736, elapsed: 6.840945720672607 seconds\n","[round 7] client: 17, loss: 0.00032491993624716997, elapsed: 6.865459680557251 seconds\n","[round 7] client: 18, loss: 0.00010185712017118931, elapsed: 6.977769374847412 seconds\n","[round 7] client: 19, loss: 0.00207566493190825, elapsed: 7.005619287490845 seconds\n","[round 7] loss: 0.25794652808690444, elapsed: 138.12479066848755 seconds\n","[round 7] Training accuracy: 82.04399871826172, Testing accuracy: 72.02999877929688\n","[round 8] client: 0, loss: 0.000266798073425889, elapsed: 6.9420390129089355 seconds\n","[round 8] client: 1, loss: 0.033656224608421326, elapsed: 6.995837211608887 seconds\n","[round 8] client: 2, loss: 0.0005180234438739717, elapsed: 6.932488918304443 seconds\n","[round 8] client: 3, loss: 0.003304852871224284, elapsed: 6.915270090103149 seconds\n","[round 8] client: 4, loss: 0.0008749112603254616, elapsed: 6.92084002494812 seconds\n","[round 8] client: 5, loss: 0.0004407463420648128, elapsed: 7.013930797576904 seconds\n","[round 8] client: 6, loss: 0.01942584663629532, elapsed: 7.007092475891113 seconds\n","[round 8] client: 7, loss: 0.00016099828644655645, elapsed: 6.850487470626831 seconds\n","[round 8] client: 8, loss: 0.003095728810876608, elapsed: 6.806796073913574 seconds\n","[round 8] client: 9, loss: 0.0060700103640556335, elapsed: 6.966259002685547 seconds\n","[round 8] client: 10, loss: 0.00043506358633749187, elapsed: 6.989231824874878 seconds\n","[round 8] client: 11, loss: 0.0002806717820931226, elapsed: 6.893687963485718 seconds\n","[round 8] client: 12, loss: 0.002215307904407382, elapsed: 6.812116861343384 seconds\n","[round 8] client: 13, loss: 0.00016165481065399945, elapsed: 6.798872470855713 seconds\n","[round 8] client: 14, loss: 0.001958842622116208, elapsed: 6.867359638214111 seconds\n","[round 8] client: 15, loss: 0.0005571560468524694, elapsed: 6.968291997909546 seconds\n","[round 8] client: 16, loss: 3.04862860502908e-05, elapsed: 6.897132158279419 seconds\n","[round 8] client: 17, loss: 0.001014673849567771, elapsed: 6.894878625869751 seconds\n","[round 8] client: 18, loss: 0.0027164388447999954, elapsed: 7.014499187469482 seconds\n","[round 8] client: 19, loss: 0.00042099421261809766, elapsed: 6.866535663604736 seconds\n","[round 8] loss: 0.07760543064250669, elapsed: 138.3570568561554 seconds\n","[round 8] Training accuracy: 83.28399658203125, Testing accuracy: 72.64999389648438\n","[round 9] client: 0, loss: 0.0005062115378677845, elapsed: 6.906178951263428 seconds\n","[round 9] client: 1, loss: 0.0013170940801501274, elapsed: 6.88045859336853 seconds\n","[round 9] client: 2, loss: 0.0025843135081231594, elapsed: 7.026130437850952 seconds\n","[round 9] client: 3, loss: 0.0006344224093481898, elapsed: 6.905640602111816 seconds\n","[round 9] client: 4, loss: 0.700958788394928, elapsed: 6.98689341545105 seconds\n","[round 9] client: 5, loss: 0.0002259403991047293, elapsed: 7.051068067550659 seconds\n","[round 9] client: 6, loss: 0.0003414816746953875, elapsed: 6.892598628997803 seconds\n","[round 9] client: 7, loss: 0.0011394922621548176, elapsed: 6.979231119155884 seconds\n","[round 9] client: 8, loss: 0.0008405969711020589, elapsed: 7.040429592132568 seconds\n","[round 9] client: 9, loss: 0.0027494842652231455, elapsed: 6.919234991073608 seconds\n","[round 9] client: 10, loss: 0.00687418133020401, elapsed: 6.9488749504089355 seconds\n","[round 9] client: 11, loss: 0.00022035150323063135, elapsed: 6.889277935028076 seconds\n","[round 9] client: 12, loss: 0.001031768973916769, elapsed: 6.9445741176605225 seconds\n","[round 9] client: 13, loss: 0.0002513282233849168, elapsed: 7.053460121154785 seconds\n","[round 9] client: 14, loss: 0.0009815845405682921, elapsed: 7.0012500286102295 seconds\n","[round 9] client: 15, loss: 0.004289461299777031, elapsed: 6.819072961807251 seconds\n","[round 9] client: 16, loss: 1.2844635421060957e-05, elapsed: 6.720504283905029 seconds\n","[round 9] client: 17, loss: 0.014990069903433323, elapsed: 6.8039820194244385 seconds\n","[round 9] client: 18, loss: 0.003401771653443575, elapsed: 7.109730005264282 seconds\n","[round 9] client: 19, loss: 0.00596947968006134, elapsed: 6.9678051471710205 seconds\n","[round 9] loss: 0.7493206672461383, elapsed: 138.84935641288757 seconds\n","[round 9] Training accuracy: 84.4280014038086, Testing accuracy: 73.00999450683594\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mu5-kbBKVfgQ"},"source":["Now let's take a look at how both models compare on the testing data. You will see that the two models are very close in performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8cfAKRK6Lp8","executionInfo":{"status":"ok","timestamp":1618194507108,"user_tz":240,"elapsed":3878,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"2a608671-5e21-4630-9a78-7f66a051c45a"},"source":["test_acc_fl = compute_model_accuracy(model_fl_global, testloader_cifar10, device=device)\n","test_acc_trad = compute_model_accuracy(model_trad, testloader_cifar10, device=device)\n","print(f\"Traditional accuracy: {test_acc_trad}\")\n","print(f\"  Federated accuracy: {test_acc_fl}\")"],"execution_count":165,"outputs":[{"output_type":"stream","text":["Traditional accuracy: 74.27999877929688\n","  Federated accuracy: 73.00999450683594\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mJb3LdoP0Chj"},"source":["Recall before that the traditional model was trained using 50,000 images, comprising a size of roughly 619.38 MB. The following cell computes the total training size and model parameter size for each of the clients."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2BDW9QIV4We","executionInfo":{"status":"ok","timestamp":1618194680015,"user_tz":240,"elapsed":15633,"user":{"displayName":"Alexander DeRieux","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXMvSXQtaWB75iKecg6p0SbX3z5jmeUWh8Esnzkg=s64","userId":"11983099067437663618"}},"outputId":"162507f7-d384-4f58-ecd4-a98865a06b79"},"source":["# Compute total image training size for each client.\n","trainset_cifar10_fl_len\n","# print([len(split.indices) for split in trainset_cifar10_fl_splits])\n","# size_cifar10_train_fl = [sum(sys.getsizeof(img.storage()) + sys.getsizeof(lbl) for img,lbl in trainset_cifar10[split.indices]) for split in trainset_cifar10_fl_splits]\n","size_cifar10_train_fl = [sum(sys.getsizeof(trainset_cifar10[idx][0].storage()) + sys.getsizeof(trainset_cifar10[idx][1]) for idx in split.indices) for split in trainset_cifar10_fl_splits]\n","size_model_fl_clients = [sum(sys.getsizeof(p.storage()) for p in model.parameters()) for model in model_fl_clients]\n","size_model_fl_global = sum(sys.getsizeof(p.storage()) for p in model_fl_global.parameters())\n","print(f\"Federated Learning model summary:\")\n","print(f\"\\tGlobal Model: model_size={size_model_fl_global:.4e} bytes\")\n","for c in range(n_clients):\n","    print(f\"\\tClient {c}: num_train_images={len(trainset_cifar10_fl_splits[c].indices)}, train_image_size={size_cifar10_train_fl[c]:.4e} bytes, model_size={size_model_fl_clients[c]:.4e} bytes\")"],"execution_count":166,"outputs":[{"output_type":"stream","text":["Federated Learning model summary:\n","\tGlobal Model: model_size=1.6866e+06 bytes\n","\tClient 0: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 1: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 2: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 3: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 4: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 5: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 6: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 7: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 8: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 9: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 10: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 11: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 12: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 13: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 14: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 15: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 16: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 17: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 18: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n","\tClient 19: num_train_images=2500, train_image_size=3.0969e+07 bytes, model_size=1.6866e+06 bytes\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TW5-NYuFf4IZ"},"source":["From the above we see that each client model was trained using 2500 images, with a total training size of roughly 30.97 MB. We also see that, like the traditional model, the parameters for each client model comprises a size of 1.68 MB. This is very promising as the federated model was trained using far less communication resources during each aggregation step (i.e., each client only transmits 1.68 MB instead of 30.97 MB).\n","\n","Using federated learning we have dramatically reduced the amount of data required to train the global model at each epoch step. By distributing the training amongst client models, each client only needs to transmit 1.68 MB worth of parameters to be aggregated. This is ideal for low-bandwidth environments, and could be decreased further if compression techniques are performed.\n","\n","This is also an excellent example data privacy, because in federated learning none of the training data is exposed to the global model, and by extension other sibling clients as well."]},{"cell_type":"code","metadata":{"id":"M1tmz70teW5N"},"source":[""],"execution_count":null,"outputs":[]}]}